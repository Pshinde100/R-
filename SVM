## Support vector machine

set.seed(1)

#simulate our own dataset

x <- matrix(rnorm(20*2), ncol = 2)
x
y <- c(rep(-1,10), rep(1,10))
y
x[y == 1,] = x[y == 1,] + 1

plot(x, col = (3 - y))

library(e1071)
dat <- data.frame(x = x, y = as.factor(y))
dat
svm.fit <- svm(y ~. , data = dat, kernel = "linear", cost = 10, scale = F)
plot(svm.fit, dat)
summary(svm.fit)


svm.fit2 <- svm(y ~ . ,data = dat, kernel = "linear", cost = 0.1, scale = F)
plot(svm.fit2, dat)
summary(svm.fit2)


set.seed(1)
tune.out <- tune(svm, y ~., data = dat, kernel = "linear", 
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10 ,100)))
summary(tune.out)
tune.out$best.model

## Predicting the values for test set using the best model

xtest <- matrix(rnorm(20*2), ncol = 2)
ytest <- sample(c(1,-1), 20, rep = T)
xtest[ytest == 1, ] = xtest[ytest == 1,] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))

yhat <- predict(tune.out$best.model, testdat) 
library(caret)

confusionMatrix(yhat, testdat$y)

## changing kernels for SVM

## generate some test data
set.seed(1)
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] = x[1:100,] + 2
x[101:150,] = x[101:150,] - 2

y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x =x , y = as.factor(y))
plot(x ,col = y)


### randomly split the data into training and test sets and fit the radial kernel

train <- sample(200, 100)
svm.fit <- svm(y ~., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
plot(svm.fit, dat[train, ])
summary(svm.fit)
yhat <- predict(svm.fit, dat[-train,])
confusionMatrix(yhat, dat[-train, "y"])

## we can see from the figure that there are a fair number of training errors
## for this fit . If we increase the value of cost. we can reduced the training 
## errors, but we risk overfitting the data

svm.fit <- svm(y ~., dat[train,], kernel = "radial", gamma = 1, cost = 1e5)
plot(svm.fit, dat[train,])

## this is certainly very irregular, and probably would overfit the data,
## lets try cross validating these parameter insted

set.seed(1)
tune.out <- tune(svm , y~., data = dat[train,], kernel = "radial",
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                 gamma = c(0.5, 1,2,3,4)))
summary(tune.out)
summary(tune.out$best.model)
yhat <- predict(tune.out$best.model, dat[-train,])
confusionMatrix(yhat, dat[-train, "y"])


## ROC plot for SVM

library(ROCR)

## while buliding h=the SVM in order to obtain a roc plot it is necessary ror
## us that we set the "Decision value = T" to obtain the fitted value

svm.opt <- svm(y~., data = dat[train,], kernel = 'radial', gamma = 2 ,
               cost = 1, decision.values = T)
fitted <- attributes(predict(svm.opt, dat[train,],
                             decision.values = T))$decision.values
rocplot(fitted, dat[train, 'y'], main = 'trainingdata')

##function for ROC plot##

rocplot <- function(pred, truth, ...){
        predob = prediction(pred,truth)
        perf = performance(predob, 'tpr', 'fpr')
        plot(perf,...)
}

